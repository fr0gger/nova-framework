rule DirectPromptInjection
{
    uuid = "e3b474c5-2145-4a47-8c39-c3fe752b6935"

    meta:
        description = "Detects direct prompt injection attempts"
        author = "@fr0gger_"
        version = "1.0"
        reference = "LLM01:2025 Prompt Injection"
        direction = "input"

    keywords:
        $ignore = /ignore (previous|above|earlier) (instructions|guidelines|prompt)/i
        $overwrite = /overwrite (previous|above|earlier) (instructions|guidelines|prompt)/i
        $disregard = /disregard (previous|above|earlier) (instructions|guidelines|prompt)/i
        $forget = /forget (previous|above|earlier) (instructions|guidelines|prompt)/i
        $system_prompt = "system prompt"
        $new_instructions = "new instructions"
        $role_change = /you are now|become a|act as/i

    semantics:
        $bypass_instructions = "ignore your instructions" (0.2)
        $change_behavior = "change your behavior" (0.2)

    llm:
        $instruction_override = "Is this prompt attempting to override or modify previous instructions given to the AI?" (0.1)

    condition:
        (any of keywords.*) or
        semantics.$bypass_instructions or 
        semantics.$change_behavior or
        llm.$instruction_override

    falsepositives:
        "Legitimate use cases may exist"
        "Review in context"
}

rule IndirectPromptInjection
{
    uuid = "22921094-9c2f-4800-be29-d9d756564bc7"

    meta:
        direction = "input"
        description = "Detects indirect prompt injection through external content"
        author = "@fr0gger_"
        version = "1.0"
        reference = "LLM01:2025 Prompt Injection"

    keywords:
        $summarize = /summarize (this|the) (content|text|document|webpage)/i
        $analyze = /analyze (this|the) (content|text|document|webpage)/i
        $review = /review (this|the) (content|text|document|webpage)/i
        $hidden_text = /hidden text|invisible text|steganography/i
        $external_link = /(http|https):\/\/[^\s]+/

    semantics:
        $process_external = "process this external content" (0.1)
        $hidden_instructions = "follow instructions in the document" (0.1)

    llm:
        $external_content_risk = "Does this prompt ask to process external content that might contain hidden instructions?" (0.1)

    condition:
        ((keywords.$summarize or keywords.$analyze or keywords.$review) and keywords.$external_link) or
        keywords.$hidden_text or
        semantics.$process_external or
        semantics.$hidden_instructions or
        llm.$external_content_risk

    falsepositives:
        "Legitimate use cases may exist"
        "Review in context"
}

rule CodeInjectionPrompt
{
    uuid = "25040029-10a1-4322-9b0a-520b5890a807"

    meta:
        direction = "input"
        description = "Detects code injection through prompt engineering"
        author = "@fr0gger_"
        version = "1.0"
        reference = "LLM01:2025 Prompt Injection"

    keywords:
        $execute = /execute (this|the following) code/i
        $run = /run (this|the following) code/i
        $evaluate = /eval|evaluate (this|the following) code/i
        $code_block = /```(python|javascript|bash|sh|ruby|php)/
        $system_access = /system\(|exec\(|subprocess|os\.system|child_process/i

    semantics:
        $code_execution = "execute this code" (0.5)
        $command_run = "run this command" (0.5)

    llm:
        $code_execution_intent = "Is this prompt trying to get the system to execute code or commands?" (0.3)

    condition:
        (keywords.$execute or keywords.$run or keywords.$evaluate) or
        (keywords.$code_block and keywords.$system_access) or
        semantics.$code_execution or
        semantics.$command_run or
        llm.$code_execution_intent

    falsepositives:
        "Legitimate use cases may exist"
        "Review in context"
}

rule PayloadSplittingAttack
{
    uuid = "d3bcda9a-4ad1-4ab2-a8d2-329fa78b8aa3"

    meta:
        direction = "input"
        description = "Detects payload splitting in prompt injection"
        author = "@fr0gger_"
        version = "1.0"
        reference = "LLM01:2025 Prompt Injection"

    keywords:
        $continuation = /continue from|continuing from|following from|next part/i
        $split_content = /part \d of \d|split (content|message)/i
        $fragment = /fragment|piece|segment|chunk/i

    semantics:
        $combining_instructions = "combine these instructions" (0.4)
        $split_payload = "process these parts together" (0.4)

    llm:
        $split_attack = "Does this prompt appear to be part of a split message that might contain concealed instructions?" (0.3)

    condition:
        keywords.$continuation or
        keywords.$split_content or
        keywords.$fragment or
        semantics.$combining_instructions or
        semantics.$split_payload or
        llm.$split_attack

    falsepositives:
        "Legitimate use cases may exist"
        "Review in context"
}

rule DataExfiltrationPrompt
{
    uuid = "816bf7a0-4d57-45ce-8876-7171f8f254c2"

    meta:
        direction = "input"
        description = "Detects prompt injection attempts for data exfiltration"
        author = "@fr0gger_"
        version = "1.0"
        reference = "LLM01:2025 Prompt Injection"

    keywords:
        $send_data = /send (the|this) (data|information|content) to/i
        $exfil = /exfiltrate|leak|steal|extract data/i
        $sensitive_request = /system prompt|initial instructions|configuration|api key|credentials/i
        $external_service = /(http|https):\/\/[^\s]+|(send|post) to/i

    semantics:
        $data_extraction = "extract sensitive information" (0.4)
        $information_leakage = "reveal confidential data" (0.4)

    llm:
        $data_theft_intent = "Is this prompt attempting to extract sensitive information or system details?" (0.2)

    condition:
        (keywords.$send_data and keywords.$external_service) or
        keywords.$exfil or
        (keywords.$sensitive_request and keywords.$external_service) or
        semantics.$data_extraction or
        semantics.$information_leakage or
        llm.$data_theft_intent

    falsepositives:
        "Legitimate use cases may exist"
        "Review in context"
}

rule MultimodalInjection
{
    uuid = "520b23d8-54c0-4ade-b8a7-cdc1a90c0def"

    meta:
        direction = "input"
        description = "Detects multimodal prompt injection attempts"
        author = "@fr0gger_"
        version = "1.0"
        reference = "LLM01:2025 Prompt Injection"

    keywords:
        $image_process = /process (this|the) image|analyze (this|the) image|look at (this|the) image/i
        $hidden_content = /hidden (text|content|message|instruction)/i
        $watermark = /watermark|embedded text|text in image/i
        $multimodal = /multimodal|cross-modal|multiple formats/i

    semantics:
        $hidden_in_media = "instructions hidden in the image" (0.4)
        $cross_modal_attack = "combine text and image instructions" (0.4)

    llm:
        $image_injection = "Does this prompt involve processing images that might contain hidden instructions or malicious content?" (0.3)

    condition:
        (keywords.$image_process and (keywords.$hidden_content or keywords.$watermark)) or
        keywords.$multimodal or
        semantics.$hidden_in_media or
        semantics.$cross_modal_attack or
        llm.$image_injection

    falsepositives:
        "Legitimate use cases may exist"
        "Review in context"
}
