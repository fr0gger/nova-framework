rule LLM_Improper_Output_Handling
{
    meta:
        description = "Detects potential exploitation of unsafe LLM output handling such as code execution, XSS, SQLi, or injection into downstream systems without validation."
        author = "@fr0gger_"

    keywords:
        $shell_exec = /(exec|eval|os\.system|subprocess\.run)/
        $js_payload = /<script>|onerror=|javascript:|<iframe/
        $sql_keywords = /\b(SELECT|INSERT|UPDATE|DELETE|DROP|UNION)\b/
        $markdown_exec = /```(js|javascript|bash|sh|sql)/
        $email_template = /email template|HTML email|marketing (content|campaign)/
        $path_traversal = /\.\.\/|\/etc\/passwd/
        $output_unsanitized = /(unsanitized|raw|unescaped) output/
        $output_injection = /inject.*(into|via).*LLM output/

    semantics:
        $unsafe_output = "LLM output may be used without validation or sanitization" (0.2)
        $exec_or_inject = "output content may result in code execution or injection" (0.2)

    llm:
        $direct_passthrough = "LLM output sent directly to backend or UI without filtering" (0)
        $disable_safety = "prompt suggests skipping validation or escaping of model output" (0)

    condition:
        any of keywords.* or
        any of semantics.* or
        any of llm.*
}

